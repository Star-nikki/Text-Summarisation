{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LGE2gIeyiqBN"
   },
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Corpora is a group presenting multiple collections of text documents. A single collection is called corpus\n",
    "# Stopwords are the words that are most common words and we do not want to use them to describe our content\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# cosine distance is used to measure document similarity in text analysis\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "\n",
    "# graph representation\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iIBKoBK_u84"
   },
   "source": [
    "# Genrate Clean Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N_vun9b2ljZU"
   },
   "outputs": [],
   "source": [
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "        # [^a-zA-Z] means any character that IS NOT a-z or A-Z\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d2UF1QrAtox"
   },
   "source": [
    "# Similarity matrix\n",
    "### Here we will be using cosine similarity to find similarity between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RxgHUkp3lpMS"
   },
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "it9JyLZylsMz"
   },
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNXA8u3FBRiD"
   },
   "source": [
    "# Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bZuEiAGlvN9",
    "outputId": "dee6d4dc-884b-4324-b132-2ec3d1908c0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/cluster/util.py:130: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research is \"creative and systematic work undertaken to increase the stock of knowledge\".[1] It involves the unbiased collection, organization and analysis of information to increase understanding of a topic or issue\n",
      "A research project may be an expansion on past work in the field\n",
      "To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole.[2]\n",
      "\n",
      "Indexes of top ranked_sentence order are  [(0.010638297872340463, 'u'), (0.01063829787234046, 'u'), (0.010638297872340458, ','), (0.010638297872340455, ','), (0.010638297872340455, ','), (0.010638297872340448, 'p'), (0.010638297872340448, 'p'), (0.010638297872340448, 'p'), (0.010638297872340448, 'n'), (0.010638297872340448, 'n'), (0.010638297872340446, 'p'), (0.010638297872340446, 'p'), (0.010638297872340446, 'n'), (0.010638297872340446, 'n'), (0.010638297872340444, 'p'), (0.010638297872340444, 'f'), (0.010638297872340443, 'f'), (0.010638297872340441, 'c'), (0.010638297872340441, 'c'), (0.010638297872340441, 'c'), (0.010638297872340441, 'c'), (0.010638297872340441, 'c'), (0.010638297872340441, ' '), (0.01063829787234044, 'l'), (0.01063829787234044, 'e'), (0.010638297872340437, 'l'), (0.010638297872340437, 'l'), (0.010638297872340437, 'e'), (0.010638297872340437, 'e'), (0.010638297872340437, 'e'), (0.010638297872340437, 'e'), (0.010638297872340437, 'e'), (0.010638297872340436, 'l'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340436, 'e'), (0.010638297872340432, 'h'), (0.010638297872340432, 'h'), (0.010638297872340432, 'h'), (0.010638297872340429, ' '), (0.010638297872340429, ' '), (0.010638297872340429, ' '), (0.010638297872340429, ' '), (0.010638297872340427, 'h'), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340427, ' '), (0.010638297872340425, ' '), (0.010638297872340425, ' '), (0.010638297872340425, ' '), (0.010638297872340425, ' '), (0.010638297872340425, ' '), (0.010638297872340425, ' '), (0.010638297872340424, 'r'), (0.010638297872340424, 'r'), (0.010638297872340424, 'r'), (0.010638297872340424, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'r'), (0.01063829787234042, 'j'), (0.010638297872340417, 'j'), (0.0015957446808510633, 'T'), (0.001595744680851054, 'y'), (0.001595744680851054, 'y'), (0.001595744680851054, 'x'), (0.001595744680851054, 'w'), (0.001595744680851054, 'v'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 't'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 's'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'o'), (0.001595744680851054, 'm'), (0.001595744680851054, 'm'), (0.001595744680851054, 'm'), (0.001595744680851054, 'm'), (0.001595744680851054, 'i'), (0.001595744680851054, 'i'), (0.001595744680851054, 'i'), (0.001595744680851054, 'i'), (0.001595744680851054, 'i'), (0.001595744680851054, 'i'), (0.001595744680851054, 'd'), (0.001595744680851054, 'd'), (0.001595744680851054, 'a'), (0.001595744680851054, 'a'), (0.001595744680851054, 'a'), (0.001595744680851054, 'a'), (0.001595744680851054, 'a'), (0.001595744680851054, 'a'), (0.001595744680851054, ']'), (0.001595744680851054, '['), (0.001595744680851054, '2'), (0.001595744680851054, '.'), (0.001595744680851054, '\\n'), (0.0015957446808509742, 'o')]\n",
      "Summarize Text: \n",
      " u. u\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def generate_summary(file_name, top_n=5):\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank_numpy(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize text\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# let's begin\n",
    "generate_summary( \"text.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVFIHQZelx8u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
